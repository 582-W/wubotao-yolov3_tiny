{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import helper\n",
    "from onnx import numpy_helper\n",
    "from onnx import AttributeProto, TensorProto, GraphProto\n",
    "import sys,getopt\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from keras.models import * \n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications.vgg16 import preprocess_input,decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 416, 416, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 416, 416, 16) 432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 416, 416, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 416, 416, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 208, 208, 16) 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 208, 208, 32) 4608        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 208, 208, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 208, 208, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 104, 104, 32) 0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 104, 104, 64) 18432       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 104, 104, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 104, 104, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 52, 52, 64)   0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 52, 52, 128)  73728       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 52, 52, 128)  512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 52, 52, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 26, 26, 128)  0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 26, 26, 256)  294912      max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 26, 26, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 26, 26, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 13, 13, 256)  0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 13, 13, 512)  1179648     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 13, 13, 512)  2048        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 13, 13, 1024) 4718592     max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 13, 13, 1024) 4096        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 13, 13, 1024) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 13, 13, 256)  262144      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 13, 13, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 13, 13, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 13, 13, 128)  32768       leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 13, 13, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 13, 13, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 26, 26, 128)  0           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 26, 26, 384)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 13, 13, 512)  1179648     leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 26, 26, 256)  884736      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 13, 13, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 26, 26, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 13, 13, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 13, 13, 255)  130815      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 26, 26, 255)  65535       leaky_re_lu_11[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 8,858,734\n",
      "Trainable params: 8,852,366\n",
      "Non-trainable params: 6,368\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "m = load_model('/data1/yolov3_tiny_416.h5')\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1\n",
      "input_1:0\n",
      "====\n",
      "conv2d_1\n",
      "input_1:0\n",
      "====\n",
      "batch_normalization_1\n",
      "conv2d_1/convolution:0\n",
      "====\n",
      "leaky_re_lu_1\n",
      "batch_normalization_1/cond/Merge:0\n",
      "====\n",
      "max_pooling2d_1\n",
      "leaky_re_lu_1/LeakyRelu/Maximum:0\n",
      "====\n",
      "conv2d_2\n",
      "max_pooling2d_1/MaxPool:0\n",
      "====\n",
      "batch_normalization_2\n",
      "conv2d_2/convolution:0\n",
      "====\n",
      "leaky_re_lu_2\n",
      "batch_normalization_2/cond/Merge:0\n",
      "====\n",
      "max_pooling2d_2\n",
      "leaky_re_lu_2/LeakyRelu/Maximum:0\n",
      "====\n",
      "conv2d_3\n",
      "max_pooling2d_2/MaxPool:0\n",
      "====\n",
      "batch_normalization_3\n",
      "conv2d_3/convolution:0\n",
      "====\n",
      "leaky_re_lu_3\n",
      "batch_normalization_3/cond/Merge:0\n",
      "====\n",
      "max_pooling2d_3\n",
      "leaky_re_lu_3/LeakyRelu/Maximum:0\n",
      "====\n",
      "conv2d_4\n",
      "max_pooling2d_3/MaxPool:0\n",
      "====\n",
      "batch_normalization_4\n",
      "conv2d_4/convolution:0\n",
      "====\n",
      "leaky_re_lu_4\n",
      "batch_normalization_4/cond/Merge:0\n",
      "====\n",
      "max_pooling2d_4\n",
      "leaky_re_lu_4/LeakyRelu/Maximum:0\n",
      "====\n",
      "conv2d_5\n",
      "max_pooling2d_4/MaxPool:0\n",
      "====\n",
      "batch_normalization_5\n",
      "conv2d_5/convolution:0\n",
      "====\n",
      "leaky_re_lu_5\n",
      "batch_normalization_5/cond/Merge:0\n",
      "====\n",
      "max_pooling2d_5\n",
      "leaky_re_lu_5/LeakyRelu/Maximum:0\n",
      "====\n",
      "conv2d_6\n",
      "max_pooling2d_5/MaxPool:0\n",
      "====\n",
      "batch_normalization_6\n",
      "conv2d_6/convolution:0\n",
      "====\n",
      "leaky_re_lu_6\n",
      "batch_normalization_6/cond/Merge:0\n",
      "====\n",
      "max_pooling2d_6\n",
      "leaky_re_lu_6/LeakyRelu/Maximum:0\n",
      "====\n",
      "conv2d_7\n",
      "max_pooling2d_6/MaxPool:0\n",
      "====\n",
      "batch_normalization_7\n",
      "conv2d_7/convolution:0\n",
      "====\n",
      "leaky_re_lu_7\n",
      "batch_normalization_7/cond/Merge:0\n",
      "====\n",
      "conv2d_8\n",
      "leaky_re_lu_7/LeakyRelu/Maximum:0\n",
      "====\n",
      "batch_normalization_8\n",
      "conv2d_8/convolution:0\n",
      "====\n",
      "leaky_re_lu_8\n",
      "batch_normalization_8/cond/Merge:0\n",
      "====\n",
      "conv2d_11\n",
      "leaky_re_lu_8/LeakyRelu/Maximum:0\n",
      "====\n",
      "batch_normalization_10\n",
      "conv2d_11/convolution:0\n",
      "====\n",
      "leaky_re_lu_10\n",
      "batch_normalization_10/cond/Merge:0\n",
      "====\n",
      "up_sampling2d_1\n",
      "leaky_re_lu_10/LeakyRelu/Maximum:0\n",
      "====\n",
      "concatenate_1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-aad11426ad2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "for layer in m.layers:\n",
    "    print(layer.name)\n",
    "    print(layer.input.name)\n",
    "    print(\"====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = load_model('/data1/model_data/vgg16.h5')\n",
    "model = onnx.load('/data1/model_data/vgg16.h5.onnx')\n",
    "node_num = len(model.graph.node)\n",
    "\n",
    "weights = {}\n",
    "dim = {}\n",
    "for i in range (node_num):\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # obtain node's name form model graph\n",
    "    weight_name = model.graph.node[i].name\n",
    "    \n",
    " \n",
    "    # check whether current node contains weight(this include weight and bias)\n",
    "    # if node's op_type is 'Constant', the node contains weight. \n",
    "    if model.graph.node[i].op_type == \"Constant\":\n",
    "\n",
    "        # create a multiple dict step by step.\n",
    "        weights[weight_name] = {}\n",
    "        dim[weight_name] = {}            \n",
    "        \n",
    "        # obtain the weight \n",
    "        weights[weight_name] = model.graph.node[i].attribute[0].t.float_data\n",
    "        # obtain the origin shape of current weight\n",
    "        dim[weight_name] = model.graph.node[i].attribute[0].t.dims \n",
    "        # reshape the 1 dim weight to 4 dims weight if it should be. For those bias, just keep 1 dim.      \n",
    "        weights[weight_name] = np.reshape(weights[weight_name], dim[weight_name])\n",
    "        # reshape weights to keras'h .h file style. For those bias, do nothing.\n",
    "        if weights[weight_name].ndim == 4:\n",
    "            weights[weight_name] = weights[weight_name].transpose(2, 3, 1, 0)\n",
    "            keras_weight_name = weight_name[:-7]\n",
    "            keras_weights = m.get_layer(keras_weight_name).get_weights()[0]\n",
    "        elif weights[weight_name].ndim == 2:\n",
    "            keras_weight_name = weight_name[:-7]\n",
    "            keras_weights = m.get_layer(keras_weight_name).get_weights()[0]\n",
    "        else:\n",
    "            keras_weight_name = weight_name[:-5]\n",
    "            keras_weights = m.get_layer(keras_weight_name).get_weights()[1]\n",
    "        \n",
    "        if ((keras_weights == weights[weight_name]).all()):\n",
    "            print('The onnx file weight value in layer' + ' ' + weight_name + ' is same as keras file')\n",
    "        else:\n",
    "            match = True\n",
    "            print(keras_weights.shape)\n",
    "            print(weight_name)\n",
    "            for i_ in range(keras_weights.shape[0]):\n",
    "                if i_ % 1000 == 999:\n",
    "                    print(\"i_: {}\".format(i_))\n",
    "                onnx_layer_num = int(49 * i_ / keras_weights.shape[0]) + 49 * i_ % keras_weights.shape[0]\n",
    "                if (keras_weights[i_] == weights[weight_name][onnx_layer_num]).all():\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"not match !!! i_ : {}\".format(i_))\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                print('The onnx file weight value in layer' + ' ' + weight_name + ' is same as keras file')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
